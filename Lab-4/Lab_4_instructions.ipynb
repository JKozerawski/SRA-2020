{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_4_instructions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXrsPstyAEA",
        "colab_type": "text"
      },
      "source": [
        "# Lab 4\n",
        "Siamese CNN for miniImageNet dataset\n",
        "\n",
        "You can find some basic information about Siamese Network [here](https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e)\n",
        "\n",
        "## Useful links:\n",
        "\n",
        "Information on Pytorch layers: [link](https://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "And more specifically:\n",
        "\n",
        "\n",
        "*   Linear layers: [link](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
        "*   Loss layers: [link](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "*   Activation functions: [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
        "*   Datasets and dataloaders: [link](https://pytorch.org/docs/stable/data.html)\n",
        "*   Saving and loading models: [link](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "*   Data transforms: [link](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
        "\n",
        "## Few-shot testing\n",
        "\n",
        "**n-way k-shot** testing means:\n",
        "\n",
        "\n",
        "*   Our testing scenario has **n** categories\n",
        "*   Every category has **k** images that we can use for training (we know which category they belong to)\n",
        "\n",
        "## glob function\n",
        "If we want to get the content of a folder to a list:\n",
        "```\n",
        "content_list = glob(\"folder_path/*\")\n",
        "```\n",
        "\n",
        "## Splitting strings:\n",
        "You can split a string into a list given some delimeter:\n",
        "Split by whitespace:\n",
        "```\n",
        "list_of_strings = single_string.split()\n",
        "```\n",
        "Split by comma:\n",
        "```\n",
        "list_of_strings = single_string.split(\",\")\n",
        "```\n",
        "\n",
        "## Numpy operations\n",
        "\n",
        "###numpy.where()\n",
        "usuful to find all indices where a certain condition is met:\n",
        "to find all indices where we have value 1 in our_array:\n",
        "```\n",
        "indices = numpy.where(our_array == 1)[0]\n",
        "```\n",
        "\n",
        "### numpy.delete()\n",
        "To delete elements from an array by index:\n",
        "```\n",
        "new_array = numpy.delete(old_array, indices_to_be_deleted)\n",
        "```\n",
        "\n",
        "### numpy.random.choice()\n",
        "Method of choosing multiple elements from an array (with a boolean possibility to indicate if the elements can repeat or not)\n",
        "```\n",
        "chosen_elements = numpy.random.choice(array, how_many_to_choose, repeat_or_not)\n",
        "```\n",
        "\n",
        "# Accessing elements inside a defined network (PyTorch):\n",
        "if the network is defined as class and has any fields defined using keyword **self** we can later refer to them.\n",
        "If we have created an object of class Net() and named it ```NN``` and the Net had a field ```self.anything``` we can later refer to it as ```NN.anything```.\n",
        "\n",
        "\n",
        "# Saving and loading models\n",
        "Keeping progress of our training is very important. Being able to save and load our previous models will become very helpful.\n",
        "\n",
        "Working on entire model:\n",
        "```\n",
        "PATH = \"./model_path.pt\"\n",
        "# saving entire model:\n",
        "torch.save(model, PATH)\n",
        "# loading entire model:\n",
        "model = torch.load(PATH)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0EW0WB_pyaI",
        "colab_type": "text"
      },
      "source": [
        "# INSTRUCTIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLzZsKoxM97S",
        "colab_type": "text"
      },
      "source": [
        "Copy this notebook to your Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXmvX5uXajB",
        "colab_type": "text"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0S4wevQXY0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6_w7cFj-TGm",
        "colab_type": "text"
      },
      "source": [
        "If you haven't already - create a folder in your Google Drive named \"data\".\n",
        "\n",
        "You can do it either manually or as command line:\n",
        "```\n",
        "%cd /content/gdrive/My\\ Drive/\n",
        "%mkdir data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEsOmGaI9Fpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# general path:\n",
        "data_path = \"/content/gdrive/My\\ Drive/data/miniImageNet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4K7gCDXsLs",
        "colab_type": "text"
      },
      "source": [
        "Follow the directions from [here](https://mtl.yyliu.net/download/) to download the miniImageNet dataset. You should download 3 .tar files: train.tar, val.tar and test.tar.\n",
        "\n",
        "Put them in the ```/content/gdrive/My\\ Drive/data/miniImageNet/``` folder on your Google Drive. If you don't have such folder, create it first.\n",
        "\n",
        "Move to that folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOC25sFyXulw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# go to the folder:\n",
        "%cd /content/gdrive/My\\ Drive/data/miniImageNet\n",
        "# print out the content of the folder:\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88jTsSCI8ah8",
        "colab_type": "text"
      },
      "source": [
        "Now untar the files. This might take few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqhceByR8_gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Untar each file:\n",
        "!tar -xvf train.tar\n",
        "!tar -xvf val.tar\n",
        "!tar -xvf test.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuNMweurjuod",
        "colab_type": "text"
      },
      "source": [
        "Confirm correct creation of 3 folders (train, test and val):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5f37CvljrJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ca297cc5-532e-4263-b936-de906ee77e5d"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet.py    \u001b[0m\u001b[01;34mtest\u001b[0m/     test.txt  train.tar  \u001b[01;34mval\u001b[0m/     val.txt\n",
            "\u001b[01;34m__pycache__\u001b[0m/  test.tar  \u001b[01;34mtrain\u001b[0m/    train.txt  val.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd5stBPKYDXE",
        "colab_type": "text"
      },
      "source": [
        "Create 3 txt files describing our data.\n",
        "Name them **train.txt**, **val.txt** and **test.txt**.\n",
        "\n",
        "Each file will have (in a separate line) information about a path to an image followed by a whitespace and an integer describing the label that the image belongs to.\n",
        "\n",
        "Example:\n",
        "```\n",
        "path_to_image_1.jpg 0\n",
        "path_to_image_2.jpg 0\n",
        "...\n",
        "...\n",
        "path_to_image_124.jpg 7\n",
        "```\n",
        "\n",
        "Modify the code below to do that. Ensure that all folders have exactly 600 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_P0lZuk9kIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "\n",
        "def folder_to_txt(split = \"train\"):\n",
        "\n",
        "  print(\"Inside:\", split, \"split\")\n",
        "  # empty string to keep track of the data:\n",
        "  txt = \"\"\n",
        "\n",
        "  # get the list of folders inside the current \"split folder\":\n",
        "  folder_list = glob(\"./\"+split+\"/*\")\n",
        "  n_categories = len(folder_list)\n",
        "\n",
        "  # enumerate through the folders:\n",
        "  for i, folder_name in enumerate(folder_list):\n",
        "    # get list of images in the current folder:\n",
        "    img_list = None   # TO BE IMPLEMENTED (using glob function)\n",
        "    n_images = len(img_list)\n",
        "\n",
        "    print(folder_name,\"Category:\", i, \"Found:\", n_images, \"images\")\n",
        "\n",
        "    # save information on the images into the text file\n",
        "    for img in img_list:\n",
        "      txt += None     # TO BE IMPLEMENTED (add information about the current image -> path and label according to the example above. Info on every image should be in a different line.)\n",
        "  # save all information to a text file\n",
        "  print(\"Saving...\")\n",
        "  txt_file = open(\"./\"+split+\".txt\",\"w+\")\n",
        "  txt_file.writelines(txt) \n",
        "  txt_file.close()\n",
        "\n",
        "\n",
        "folder_to_txt(\"train\")\n",
        "folder_to_txt(\"val\")\n",
        "folder_to_txt(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hWirg1yAEB",
        "colab_type": "text"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqIfVxocyAED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1NH-E--Ap5",
        "colab_type": "text"
      },
      "source": [
        "Create a custom dataset where you will use your text files to store information about your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9OFI9KO-IZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data transforms:\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(84),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(84),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(84),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Create your custom dataset below:\n",
        "class Siamese_Dataset(Dataset):\n",
        "    \n",
        "    def __init__(self, txt, transform=None):\n",
        "        # prepare two lists where we will keep track of image paths and their labels\n",
        "        self.img_path = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # read the txt file into our empty lists:\n",
        "        with open(txt) as f:\n",
        "            for line in f:\n",
        "                # TO BE IMPLEMENTED\n",
        "                # Append correct information to the lists.\n",
        "                # self.img_path should contain only paths to images (as strings)\n",
        "                # self.labels should contain labes (as integers)\n",
        "                self.img_path.append(None)\n",
        "                self.labels.append(None)\n",
        "\n",
        "        # let's convert the list to a numpy array        \n",
        "        self.labels = np.asarray(self.labels)        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # get the path and label of an image at index 'index'\n",
        "        path = self.img_path[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # with 50% chance find an image from a different class\n",
        "        # in other 50% of cases - find a different image from the same class:\n",
        "\n",
        "        p = random.random() # random float between 0-1\n",
        "        similarity_label = 0\n",
        "        if p > 0.5:\n",
        "          # find an image from a different class\n",
        "          possible_indices = None # TO BE IMPLEMENTED (find all indices where images have different label than our current image)\n",
        "          similarity_label = 0\n",
        "        else:\n",
        "          # find a different image from the same class:\n",
        "          possible_indices =  None # TO BE IMPLEMENTED (find all indices where images have the same label as our current image)\n",
        "          # remove our current image from possible choices:\n",
        "          possible_indices = None # TO BE IMPLEMENTED (remove the index of our current image from the list)\n",
        "          similarity_label = 1\n",
        "        \n",
        "        # choose a second image at random:\n",
        "        second_index = np.random.choice(possible_indices, 1)[0]\n",
        "       \n",
        "        # get its path:\n",
        "        second_path = self.img_path[second_index]\n",
        "\n",
        "\n",
        "        # open image 1:\n",
        "        with open(path, 'rb') as f:\n",
        "            image1 = Image.open(f).convert('RGB')\n",
        "        # open image 2:\n",
        "        with open(second_path, 'rb') as f:\n",
        "            image2 = None # TO BE IMPLEMENTED (read the second image similarly to the first one)\n",
        "        if self.transform is not None:\n",
        "            image1 = self.transform(image1)\n",
        "            image2 = None # TO BE IMPLEMENTED (transform the second image similarly to the first one)\n",
        "        return [image1, image2], similarity_label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYh5DfWX_DdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # means to use all\n",
        "# how many samples per batch to load\n",
        "batch_size = 128\n",
        "\n",
        "# create training dataset\n",
        "train_data = Siamese_Dataset(\"./train.txt\", data_transforms[\"train\"])\n",
        "\n",
        "# create train and val data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TighTVKsyAEJ",
        "colab_type": "text"
      },
      "source": [
        "Define the network:\n",
        "\n",
        "We will use an existing network. Copy the ConvNet.py file from [here](https://drive.google.com/file/d/1Hf6j95u88qwJM3TnsWeiKGLTxaFHh4h4/view?usp=sharing) and put it in your \"miniImageNet\" folder on your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd547-kayAEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the module (to use the network):\n",
        "from ConvNet import Convnet\n",
        "\n",
        "# create our own network:\n",
        "class SiameseNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = Convnet()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = None # TO BE IMPLEMENTED (perform a forward pass using our convnet with x1 as input)\n",
        "        out2 = None # TO BE IMPLEMENTED (perform a forward pass using our convnet with x2 as input)\n",
        "        return out1, out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhRDTUgGUfVK",
        "colab_type": "text"
      },
      "source": [
        "Now let's specify the loss function that we will use.\n",
        "For this reason let's adapt Contrastive Loss implemented [here](https://github.com/adambielski/siamese-triplet/blob/master/losses.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsGil7YrUo43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function:\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss\n",
        "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, output1, output2, target, size_average=True):\n",
        "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
        "        losses = 0.5 * (target.float() * distances + (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
        "        return losses.mean() if size_average else losses.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pMld9zBfEz3",
        "colab_type": "text"
      },
      "source": [
        "Let's create a function that will take our model as a parameter and will choose n images from each category and generate their embeddings (extract feature vectors that describe the images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBDkmsDrgjb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function\n",
        "def read_images(indices, paths):\n",
        "  images = []\n",
        "  for idx in indices:\n",
        "    with open(paths[idx], 'rb') as f:\n",
        "      image = Image.open(f).convert('RGB')\n",
        "      image = data_transforms[\"test\"](image)\n",
        "    images.append(image)\n",
        "  return torch.stack(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT18rNBA96i-",
        "colab_type": "text"
      },
      "source": [
        "Let's read the information about the validation and test splits and store it in two lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQy7FBy-CNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's read the information about test data first and store it:\n",
        "def get_data_info(txt_file):\n",
        "  img_path = []\n",
        "  labels = []\n",
        "  with open(txt_file) as f:\n",
        "    for line in f:\n",
        "      # TO BE IMPLEMENTED\n",
        "      # Append correct information to the lists.\n",
        "      # self.img_path should contain only paths to images (as strings)\n",
        "      # self.labels should contain labes (as integers)\n",
        "      img_path.append(None)\n",
        "      labels.append(None)\n",
        "  labels = np.asarray(labels) \n",
        "  return img_path, labels\n",
        "\n",
        "val_img_path, val_labels = get_data_info(\"./val.txt\")\n",
        "test_img_path, test_labels = get_data_info(\"./test.txt\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXpAwuHy-De8",
        "colab_type": "text"
      },
      "source": [
        "Extract embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuZMZNUBfUCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, phase = \"val\", n_samples = 50, n_labels = 5, n_predict = 5):\n",
        "  # max number for n_labels: 20 for test and 16 for val\n",
        "  # max number for n_samples+n_predict: 600\n",
        "\n",
        "  # n_samples = number of testing images per category\n",
        "  # n_predict = k-shot (number of training images per category)\n",
        "  # n_labels = n-way (how many categories)\n",
        "\n",
        "  # choose different images and labels depending if we are in the validation or test phase:\n",
        "  if phase == \"val\":\n",
        "    labels = val_labels\n",
        "    img_path = val_img_path\n",
        "  else:\n",
        "    labels = test_labels\n",
        "    img_path = test_img_path\n",
        "\n",
        "  # find all unique labels in the current split (phase)\n",
        "  unique_labels = np.unique(labels)\n",
        "  # choose n_labels out of all labels (wihtout repetitions)\n",
        "  unique_labels = None # TO BE IMPLEMENTED \n",
        "\n",
        "  outputs = []\n",
        "  centroids = []\n",
        "  labels = []\n",
        "  for i, label in enumerate(unique_labels):\n",
        "    # find images with a correct label:\n",
        "    indices = np.where(labels == label)[0] \n",
        "    # choose n_samples+n_predict indices without repetition:\n",
        "    indices = None # TO BE IMPLEMENTED \n",
        "\n",
        "    images = read_images(indices, img_path)          # convert paths to images to tensors\n",
        "    output = None # TO BE IMPLEMENTED (forward pass on images -> since we have only one image (not a pair) use only one part of the Siamese network)\n",
        "    # convert to Numpy:\n",
        "    output = output.data.numpy() \n",
        "\n",
        "    # calculate the center of the support samples:         \n",
        "    center = np.mean(output[n_samples:], axis=0)\n",
        "    centroids.append(center) \n",
        "\n",
        "    # the rest of samples is our query:\n",
        "    outputs.append(output[:n_samples])\n",
        "    labels.append(i*np.ones(n_samples).astype(int))\n",
        "\n",
        "  # we have list of separate outputs (and labels) for each category. Convert it to a single array (one for outputs and one for labels):\n",
        "  embeddings = np.asarray(outputs).reshape(n_samples * n_labels, -1)\n",
        "  labels = np.asarray(labels).reshape(n_samples * n_labels)\n",
        "  centroids = np.asarray(centroids)\n",
        "  return embeddings, labels, centroids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOg4ig-hfpuu",
        "colab_type": "text"
      },
      "source": [
        "Let's create a testing routine to predict samples in the val/test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00O7jsjVfyk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nearest Category Mean\n",
        "def NCM(query, train):\n",
        "  train = torch.tensor(train)\n",
        "  query = torch.tensor(query)\n",
        "  n = query.shape[0]\n",
        "  m = train.shape[0]\n",
        "  query = query.unsqueeze(1).expand(n, m, -1)\n",
        "  train = train.unsqueeze(0).expand(n, m, -1)\n",
        "  logits = -((query - train) ** 2).sum(dim=2)\n",
        "  return logits\n",
        "\n",
        "# Calculating the confusion matrix\n",
        "def create_confusion_matrix(logits, labels):\n",
        "  n_classes = len(np.unique(labels))\n",
        "\n",
        "  confusion_matrix = None # TO BE IMPLEMENTED (create an array of zeros of size n_classses x n_classes)\n",
        "\n",
        "  # from the logits (scores) get the prediction:\n",
        "  pred = torch.argmax(logits, dim=0).numpy()\n",
        "\n",
        "  # iterate through our images:\n",
        "  for i in range(len(pred)):\n",
        "    # for each image add 1 to the confusion matrix cell depending on the image's ground truth (label) and our prediction:\n",
        "    confusion_matrix[None, None] += 1 # TO BE IMPLEMENTED \n",
        "\n",
        "  # calculate the accuracy from the confusion matrix:\n",
        "  accuracy = np.sum(np.diag(confusion_matrix))/np.sum(confusion_matrix)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBpytd08yAEh",
        "colab_type": "text"
      },
      "source": [
        "Training the network.\n",
        "\n",
        "We will iterate through our dataset. For evey iteration we need to:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhWP7P7CyAEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the model:\n",
        "model = SiameseNet()\n",
        "print(model)\n",
        "\n",
        "# specify loss:\n",
        "criterion = ContrastiveLoss(margin=1.0)\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# specify scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# number of epochs to train the model\n",
        "n_epochs = 5\n",
        "\n",
        "n_iterations = int(len(train_data)/batch_size)\n",
        "\n",
        "max_iters = 30\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    model.train() # prep model for training\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "\n",
        "    for ii, (data, target) in enumerate(train_loader):\n",
        "        if ii % 5 == 0:\n",
        "          print(\"Epoch:\", epoch, \"Iteration:\", ii, \"out of:\", n_iterations)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs1, outputs2 = model(data[0], data[1])\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs1, outputs2, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data[0].size(0)\n",
        "\n",
        "        if ii >= max_iters:\n",
        "          break\n",
        "    \n",
        "    # if you have a learning rate scheduler - perform a its step in here\n",
        "    scheduler.step()\n",
        "    # print training statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/((ii+1)*batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "    # Run the validation pass multiple times and report the average:\n",
        "    model.eval()  # prep model for testing\n",
        "    n_tests = 10\n",
        "    accuracy = 0.0\n",
        "    for i in range(n_tests):\n",
        "      embeddings, labels, centroids = test(model, phase = \"val\", n_samples = 100, n_labels = 5, n_predict = 5)\n",
        "      scores = NCM(centroids, embeddings)\n",
        "      accuracy += create_confusion_matrix(scores, labels)\n",
        "    print(\"Overall accuracy:\", 100*(accuracy/n_tests), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2kZs54yGNeY",
        "colab_type": "text"
      },
      "source": [
        "After training, test the model on the test set. Write the code for it below. Use the example of validation set as your reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_4KVbb-GePi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO BE IMPLEMENTED"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcXnBT4zmLIU",
        "colab_type": "text"
      },
      "source": [
        "## To do:\n",
        "Run each test scenario 25 times and report the average accuracy. Use the example of validation set as your reference.\n",
        "\n",
        "1.   Test the trained network in a 5-way 5-shot scenario with 100 unknown examples per each category\n",
        "2.   Test the trained network in a 20-way 5-shot scenario with 100 unknown examples per each category\n",
        "3.   Test the trained network in a 5-way 2-shot scenario with 100 unknown examples per each category\n",
        "4.   Copy a pre-trained model from [here](https://drive.google.com/file/d/1w5Pl_mmSK90UWJLon7NSDPac_FzvLD5U/view?usp=sharing) to your folder, load it and perform the same 3 test comparisons as above.\n",
        "\n",
        "In your research report - provide information on final training loss and validation accuracy. Run the 3 different testing scenarios described above (both for your model and the pre-trained one), provide the accuracies observed, and explain the differences in the observed accuracies.\n",
        "\n"
      ]
    }
  ]
}
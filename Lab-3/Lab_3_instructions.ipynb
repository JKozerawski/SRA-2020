{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Lab_3_instructions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXrsPstyAEA",
        "colab_type": "text"
      },
      "source": [
        "# Lab 3\n",
        "Convolutional Neural Networks\n",
        "\n",
        "## Useful links:\n",
        "\n",
        "Information on Pytorch layers: [link](https://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "And more specifically:\n",
        "\n",
        "\n",
        "*   Linear layers: [link](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
        "*   Loss layers: [link](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "*   Activation functions: [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
        "*   Datasets and dataloaders: [link](https://pytorch.org/docs/stable/data.html)\n",
        "*   Saving and loading models: [link](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "\n",
        "TSNE visualization: [link](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfcMenIEmic1",
        "colab_type": "text"
      },
      "source": [
        "#Information about network training:\n",
        "\n",
        "### How to use loss function:\n",
        "```\n",
        "# define the loss function once (before training):\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# to calculate our loss after the forward pass:\n",
        "current_loss = loss_function(outputs, target)\n",
        "\n",
        "# perform backward pass:\n",
        "current_loss.backward()\n",
        "```\n",
        "\n",
        "### Network optimization (learning):\n",
        "```\n",
        "# define the optimizer once (before training):\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # optimizing parameters (weights) with learning rate 0.01\n",
        "\n",
        "# before performing the backward pass clear the information about the gradients from the previous pass:\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# after performing the backward pass\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "### Forward pass:\n",
        "```\n",
        "#if you have already defined a model, the only thing you have to do is:\n",
        "outputs = model(inputs)\n",
        "```\n",
        "\n",
        "#Information about defining the network:\n",
        "\n",
        "Every model will have similar structure:\n",
        "```\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define your layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass operations here\n",
        "        return x\n",
        "```\n",
        "\n",
        "There are two distinc functions here, the ```__init__``` and ```forward```.\n",
        "\n",
        "In the ```__init__``` function we will define layers that we will later on use in our forward pass.\n",
        "\n",
        "In the ```forward``` function we will define step by step what should happen in our forward pass.\n",
        "\n",
        "# Layers\n",
        "### Linear\n",
        "First layer we can use is a linear (or fully connected) layer. We can define it as:\n",
        "```\n",
        "# Linear layer with 5 inputs and 2 outputs (goes inside the __init__)\n",
        "self.fc = torch.nn.Linear(in_features=5, out_features=2)\n",
        "\n",
        "# moving data through the layer (goes into the forward function)\n",
        "output = self.fc(input) \n",
        "```\n",
        "### Activation\n",
        "Activation functions don't have to be defined in the ```__init__``` function as long as they don't have any trainable parameters (and most of them don't have any).\n",
        "```\n",
        "# moving data through the layer with sigmoid (goes into the forward function)\n",
        "output = F.sigmoid(self.fc(input)) \n",
        "\n",
        "# moving data through the layer with relu (goes into the forward function)\n",
        "output = F.relu(self.fc(input)) \n",
        "```\n",
        "\n",
        "### Reshape\n",
        "Frequently you will have to reshape your input (from 2D to 1D for example).\n",
        "```\n",
        "# if input is of shape (N, 10, 10)\n",
        "output = input.view(-1, 100)\n",
        "# now output is of shape (N, 100)\n",
        "```\n",
        "\n",
        "### 2D Convolution\n",
        "Definition of the convolution layer\n",
        "```\n",
        "# Convolution layer with 10 filters of size 3x3. The input has 5 channels.\n",
        "self.conv = nn.Conv2d(5, 10, kernel_size=3)\n",
        "\n",
        "# Forward pass:\n",
        "output = self.conv(input)\n",
        "```\n",
        "\n",
        "### 2D max pooling\n",
        "```\n",
        "# Performing a 2D max pooling operation with a kernel of size 2x2\n",
        "output = F.max_pool2d(self.conv(input), 2)\n",
        "```\n",
        "\n",
        "### Dropout\n",
        "```\n",
        "# 1D dropout performed on the output of a linear layer\n",
        "output = F.dropout(self.linear(input))\n",
        "```\n",
        "\n",
        "# Data transformations\n",
        "```\n",
        "# simplest transformation (transforming image to PyTorch tensor):\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# you can add more transformations (after you converted image to tensor). Simplest would be normalization (for 1 channel data (grayscale)):\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((mean,), (std,))])\n",
        "```\n",
        "\n",
        "# Saving and loading models\n",
        "Keeping progress of our training is very important. Being able to save and load our previous models will become very helpful.\n",
        "\n",
        "Working on entire model:\n",
        "```\n",
        "PATH = \"./mnist_model.pt\"\n",
        "# saving entire model:\n",
        "torch.save(model, PATH)\n",
        "# loading entire model:\n",
        "model = torch.load(PATH)\n",
        "```\n",
        "\n",
        "Saving more details. Useful when stopping and resuming training.\n",
        "```\n",
        "PATH = \"./mnist_model.pt\"\n",
        "# saving more detailed information:\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            ...\n",
        "            }, PATH)\n",
        "# loading more detailed information:\n",
        "model = Net() # initialize the object first\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # initialize the object first\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0EW0WB_pyaI",
        "colab_type": "text"
      },
      "source": [
        "# INSTRUCTIONS\n",
        "\n",
        "## Task 1 - CNN for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXmvX5uXajB",
        "colab_type": "text"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0S4wevQXY0t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bfe1cbd7-7afd-4bc9-ef8c-2e3ee888f0e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6_w7cFj-TGm",
        "colab_type": "text"
      },
      "source": [
        "Create a folder in your Google Drive named \"data\".\n",
        "\n",
        "You can do it either manually or as command line:\n",
        "```\n",
        "%cd /content/gdrive/My\\ Drive/\n",
        "%mkdir data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEsOmGaI9Fpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# general path:\n",
        "data_path = \"/content/gdrive/My\\ Drive/data/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4K7gCDXsLs",
        "colab_type": "text"
      },
      "source": [
        "Move to that folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOC25sFyXulw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc5832c0-667f-4208-f7b4-2b50ec0045c4"
      },
      "source": [
        "# go to the folder:\n",
        "%cd /content/gdrive/My\\ Drive/data/\n",
        "# print out the content of the folder:\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/data\n",
            "\u001b[0m\u001b[01;34mFlowers\u001b[0m/  \u001b[01;34mMNIST\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hWirg1yAEB",
        "colab_type": "text"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqIfVxocyAED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd5stBPKYDXE",
        "colab_type": "text"
      },
      "source": [
        "Load the dataset.\n",
        "\n",
        "Modify the transforms for the dataset to include a normalization of data. Numbers for MNIST are: mean is 0.1307, std is 0.3081."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYh5DfWX_DdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Read MNIST ########\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # means to use all\n",
        "# how many samples per batch to load\n",
        "batch_size = 64\n",
        "# where the dataset is:\n",
        "dataset_path = \"./MNIST\"\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = None # modify it according to the description above. Previously was: transform = transforms.ToTensor()\n",
        "\n",
        "# create training and test datasets\n",
        "train_data = datasets.MNIST(root=dataset_path, train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root=dataset_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TighTVKsyAEJ",
        "colab_type": "text"
      },
      "source": [
        "Define the network:\n",
        "\n",
        "Your Convolutional Neural Network should have the following layers:\n",
        "\n",
        "2D convolution with 10 kernels of size 5\n",
        "2D convolution with 20 kernels of size 5\n",
        "fully connected (linear) layer with output size 50\n",
        "final fully connected layer with output size 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd547-kayAEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define your layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass operations here\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBpytd08yAEh",
        "colab_type": "text"
      },
      "source": [
        "Training the network.\n",
        "\n",
        "We will iterate through our dataset. For evey iteration we need to:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhWP7P7CyAEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the model:\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# specify scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# number of epochs to train the model\n",
        "n_epochs = 25\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "test_accuracy_progress = []\n",
        "\n",
        "model.train() # prep model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data, target in train_loader:\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    # if you have a learning rate scheduler - perform a its step in here\n",
        "    scheduler.step()\n",
        "    # print training statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "    # Run the test pass:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # prep model for testing\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    test_accuracy_progress.append(100 * correct / total)\n",
        "    print('Accuracy of the network on the test set: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeLJtM1Z1Z-D",
        "colab_type": "text"
      },
      "source": [
        "Plot the progress using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5j9PzoI1sHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting:\n",
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, test_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcXnBT4zmLIU",
        "colab_type": "text"
      },
      "source": [
        "## To do:\n",
        "Perform following modifications (one at a time) to the forward pass:\n",
        "1.   Use relu activation functions for both conv layers and for the first linear layer\n",
        "2.   Add a dropout layer between the two linear layers\n",
        "3.   Add 2D max pooling layer before performing a relu activation on both conv layers\n",
        "\n",
        "Note the differences in performances when modifying the network.\n",
        "\n",
        "In your research report - provide results and analysis of the conducted experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3oYMC0za4sm",
        "colab_type": "text"
      },
      "source": [
        "## Task 2 - Flowers dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG0I-dxxazNB",
        "colab_type": "text"
      },
      "source": [
        "Get the zip file with images from this [link](https://drive.google.com/file/d/1jf6-NFfCRJHIZWrbg-_PT5w-_xmqV_Iv/view?usp=sharing) and put it in your Google Drive in ```/data/Flowers/``` folder.\n",
        "\n",
        "So the path should look like this:\n",
        "```\n",
        "\"/content/gdrive/My\\ Drive/data/Flowers/flowers.zip\"\n",
        "```\n",
        "\n",
        "Go to the current directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy6rAIz2avL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d3c78b3-7afc-4e50-ab4b-8c580a35a385"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/data/Flowers/\n",
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/data/Flowers\n",
            "flowers.zip  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mval\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtAUNY1vbmDw",
        "colab_type": "text"
      },
      "source": [
        "Unzip the file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZthfJUK8bp0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%unzip ./flowers.zip -d ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ77URc3d3is",
        "colab_type": "text"
      },
      "source": [
        "Check if the folders are there. You should see 3 folders (train, val and test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMeu1h5_d-6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-qEFP1dF3O",
        "colab_type": "text"
      },
      "source": [
        "Imports and dataloaders:\n",
        "\n",
        "Implement a following transformations to your data:\n",
        "\n",
        "For test and validation data:\n",
        "\n",
        "*   Use a RandomResiedCrop to crop image to size 224x224\n",
        "*   Perform a random horizontal flip\n",
        "*   Perform a random rotation by max 10 degrees\n",
        "*   Perform a color jitter with parameters: brightness=0.4, contrast=0.4, saturation=0.4, hue=0\n",
        "*   Make it a tensor\n",
        "*   Normalize it using the following data: means: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]\n",
        "\n",
        "For test and validation data:\n",
        "\n",
        "*   Resize to 256x256 img size\n",
        "*   Crop the center of the image of size 224x224\n",
        "*   Make it a tensor\n",
        "*   Normalize it using the following data: means: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHITgW-dTaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 64\n",
        "num_workers = 0\n",
        "\n",
        "# define transforms:\n",
        "train_transform = None\n",
        "test_transform = None\n",
        "\n",
        "# define datasets:\n",
        "train_data = datasets.ImageFolder(\"./train\", transform=train_transform)\n",
        "val_data = datasets.ImageFolder(\"./val\", transform=test_transform)\n",
        "test_data = datasets.ImageFolder(\"./test\", transform=test_transform)\n",
        "\n",
        "# define dataloaders:\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meZ3rmYVe85-",
        "colab_type": "text"
      },
      "source": [
        "Define your network\n",
        "\n",
        "List of predefined models: [link](https://pytorch.org/docs/stable/torchvision/models.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vACxdBxae_mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJD-UU1DpAKB",
        "colab_type": "text"
      },
      "source": [
        "Remove the last layer of the network. Add to it your own layer (with the output dimention corresponding to number of categories in your dataset). In this case it is 102."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sThfnnC5pNXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace the last fc layer with your own linear layer:\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kdr5EQZpdms",
        "colab_type": "text"
      },
      "source": [
        "Train (and validate):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lCyWzVMpbw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# specify scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n",
        "\n",
        "model.train() # prep model for training\n",
        "\n",
        "n_iterations = int(len(train_data)/batch_size)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for iter, (data, target) in enumerate(train_loader):  \n",
        "        print(\"Epoch:\", epoch, \"Iteration:\", iter, \"out of:\", n_iterations)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    # if you have a learning rate scheduler - perform a its step in here\n",
        "    scheduler.step()\n",
        "    # print training statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "    # Run the test pass:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # prep model for validation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC4X1kRisXZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the test pass:\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()  # prep model for validation\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQaHfMjk0x0b",
        "colab_type": "text"
      },
      "source": [
        "## To do:\n",
        "\n",
        "1.   Write a part where you save your model after you're finished training\n",
        "2.   Train the network above. Provide the training loss, validation accuracy and test accuracy at the end.\n",
        "3.   Change the network to be pretrained. Decrease the initial learning rate to be 0.001. Train the network this way and provide same performance measures.\n",
        "\n",
        "Compare the impact the use of a pretrained network had on the final accuracy and on the training in general."
      ]
    }
  ]
}